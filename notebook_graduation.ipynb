{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59992de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0471b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Admission_Predict_Ver1.1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74272073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 9)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41ff4918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of      Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "495         496        332          108                  5  4.5   4.0  9.02   \n",
       "496         497        337          117                  5  5.0   5.0  9.87   \n",
       "497         498        330          120                  5  4.5   5.0  9.56   \n",
       "498         499        312          103                  4  4.0   5.0  8.43   \n",
       "499         500        327          113                  4  4.5   4.5  9.04   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "495         1              0.87  \n",
       "496         1              0.96  \n",
       "497         1              0.93  \n",
       "498         0              0.73  \n",
       "499         0              0.84  \n",
       "\n",
       "[500 rows x 9 columns]>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6359a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deca971b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df=df.iloc[:,1:]\n",
    "df.drop(columns=['Serial No.'],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c73330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36545ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "358902c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>310</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>300</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "238        310          104                  3  2.0   3.5  8.37         0\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "475        300          101                  3  3.5   2.5  7.88         0\n",
       "58         300           99                  1  3.0   2.0  6.80         1\n",
       "380        322          104                  3  3.5   4.0  8.84         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a33af89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4       , 0.42857143, 0.5       , ..., 0.57142857, 0.50320513,\n",
       "        0.        ],\n",
       "       [0.56      , 0.64285714, 0.        , ..., 0.57142857, 0.55769231,\n",
       "        1.        ],\n",
       "       [0.2       , 0.32142857, 0.5       , ..., 0.28571429, 0.34615385,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.74038462,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.77884615,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.32051282,\n",
       "        0.        ]], shape=(400, 7))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9761491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b9a70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shubh Agnihotri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(7,activation='relu',input_dim=7))\n",
    "model.add(Dense(14,activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4dbec56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │           \u001b[38;5;34m112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183</span> (732.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m183\u001b[0m (732.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183</span> (732.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m183\u001b[0m (732.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5764a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='Adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a1e2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2343 - val_loss: 0.0627\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0210 - val_loss: 0.0164\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0120 - val_loss: 0.0123\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0112\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0101\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0092 - val_loss: 0.0093\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0051 - val_loss: 0.0052\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0050\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0049 - val_loss: 0.0051\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0039\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0041\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0040\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 69/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0041 - val_loss: 0.0036\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0038 - val_loss: 0.0036\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036 - val_loss: 0.0036\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0038 - val_loss: 0.0034\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0037 - val_loss: 0.0035\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train_scaled,y_train,validation_split=0.2,epochs=100,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4d4fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C517733420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001C517733420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "87205ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7955730431514193"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de1b5ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c514e34090>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
